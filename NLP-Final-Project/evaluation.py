import tensorflow as tf
from data_processing import *
from utile import *
from math import ceil
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

def predict_lr(model, pred_vec, abvs, abv_dict):
    """
    :param model: Logistic Regression model
    :param pred_vec: list[list[int]]; List of validation or testing vectors 
    :param abvs: list[str]; List of abbreviations 
    :param abv_dict: dict{str, str}; Dictionnary of expansions (keys) to abbreviations (values)
    :return: result_preds: list[str]; List of predicted abbreviation expansions  
    Predicts expansions according to prediciton vectors and model.
    """
    pred_probs = model.predict_proba(pred_vec)
    
    result_preds = []

    for pred_prob, abv in zip(pred_probs, abvs):
        # Map expansions to probabilities 
        probs = dict(zip(model.classes_, pred_prob))
        # Sort for expansions with highest probabilities
        probs_sorted = {k: v for k, v in sorted(probs.items(), key=lambda item: item[1], reverse=True)}
        # Take expantion with matching abbreviation and highest probability
        for class_, prob in probs_sorted.items():
            if abv_dict[class_] == abv:
                result_preds.append(class_)
                break
    
    return result_preds

 
    
def evaluate(model, x_test, y_test, exp_range_test):
    """
    :param x_test: np.array[str]
    :param y_test: np.array[int]
    :param exp_range_test: list[list[int]]; The possible expansions for each test case
    Evaluate the accuracy.
    Each prediction is generated by:
    1. Generate probability distribution over all labels.
    2. Select the label from the possible expansion range with highest probability
    """
    batch_size = 5000
    cnt = 0
    range_total_size = 1
    for j in range(ceil(len(x_test) / batch_size)):
        start = j * batch_size
        end = min(len(x_test), start + batch_size)
        print('\r',
              f'{end} of {len(x_test)} evaluated, acc: {cnt / end}  range avg size: {range_total_size / end}...  ',
              end='')
        probabilities = model.predict(x_test[start:end])

        for i, res in enumerate(probabilities):
            exp_range = exp_range_test[start + i]
            max_p = float('-inf')
            max_idx = -1
            range_total_size += len(exp_range)
            for idx in exp_range:
                if res[idx] > max_p:
                    max_p = max_p
                    max_idx = idx
            if y_test[start + i] == max_idx:
                cnt += 1
    print('Done')
    print(f'{cnt} out of {len(y_test)} correct, acc: {cnt / len(y_test)}')

def evaluate_lr(model, valid_test, abv_dict, vect):
    """
    :param model: Logistic Regression model
    :param valid_test: Dataframe; Processed validation or testing DataFrame 
    :param vect: Doc2Vec model trained and used to predict abbreviation expansions
    :param abv_dict: dict{str, str}; Dictionnary of expansions (keys) to abbreviations (values)
    Predicts expansions according to prediction vectors and model.
    Prints accuracy and F1 score.   
    """
    
    valid_test_tagged = valid_test.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)
    
    y_valid_test = [doc.tags[0] for doc in valid_test_tagged.values]    
    X_valid_test = [vect.infer_vector(doc.words) for doc in valid_test_tagged.values]  
    
    y_pred = predict_lr(model, X_valid_test, valid_test.ABV, abv_dict)
    
    print("Done")
    print('Validation Accuracy:', accuracy_score(y_valid_test, y_pred))
    print('Validation or Testing F1-Score:', f1_score(y_valid_test, y_pred, average='weighted'))
    
    valid_test["Prediction"] = y_pred
    valid_test["Target"] = y_valid_test
    valid_test.to_csv("Logistic_Regression_Results7.csv")


def evaluate_rf(model, valid_test, vect):
    """
    :param model: Random Forest model
    :param valid_test: Dataframe; Processed validation or testing DataFrame 
    :param vect: Doc2Vec model trained and used to predict abbreviation expansions
    Predicts expansions according to articles and model.
    Prints accuracy and F1 score.   
    """
    valid_test_tagged = valid_test.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)
    
    y_valid_test = [doc.tags[0] for doc in valid_test_tagged.values]    
    X_valid_test = [vect.infer_vector(doc.words) for doc in valid_test_tagged.values]  
    
    y_pred = model.predict(X_valid_test)

    print("Done")
    print('Validation Accuracy:', accuracy_score(y_valid_test, y_pred))
    print('Validation F1-Score:', f1_score(y_valid_test, y_pred, average='weighted'))
    
    valid_test["Prediction"] = y_pred
    valid_test["Target"] = y_valid_test
    valid_test.to_csv("Random_Forest_Results4.csv")


if __name__ == '__main__':
    model = tf.keras.models.load_model(os.path.join('..', 'Models', 'my_model'))
    x_test = pickle.load(open(os.path.join('..', 'Data', 'x_test.pkl'), "rb"))
    y_test = pickle.load(open(os.path.join('..', 'Data', 'y_test.pkl'), "rb"))
    exp_range_test = pickle.load(open(os.path.join('..', 'Data', 'exp_range_test.pkl'), "rb"))
    evaluate(model, x_test, y_test, exp_range_test)
